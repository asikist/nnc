{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT-LTI: Multi-sample Training and Eval\n",
    "In this notebook we train over different graphs and initial-target state pairs.\n",
    "We change parametrization slightly from the single sample, using Xavier normal instead of Kaiming initialization and higher decelaration rate for training. Preliminary results on few runs indicated the above choices would lead to faster convergence on BA and Tree graphs. Still, extensive hyper-parameter optimization would be preferable in the future, especially to optimize performance further.\n",
    "\n",
    "Please make sure that the required data folder is available at the paths used by the script.\n",
    "You may generate the required data by running the python script\n",
    "```nodec_experiments/ct_lti/gen_parameters.py```.\n",
    "\n",
    "This notebook takes around 15 hours per graph on RTX TITAN GPU, so plesae be patient when you generate new results.\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append('../../../')\n",
    "\n",
    "import torch\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm.cli import tqdm\n",
    "\n",
    "from nnc.controllers.baselines.ct_lti.dynamics import ContinuousTimeInvariantDynamics\n",
    "from nnc.controllers.baselines.ct_lti.optimal_controllers import ControllabiltyGrammianController\n",
    "\n",
    "from nnc.helpers.torch_utils.graphs import adjacency_tensor, drivers_to_tensor\n",
    "from nnc.helpers.graph_helper import load_graph\n",
    "from nnc.helpers.torch_utils.evaluators import FixedInteractionEvaluator\n",
    "from nnc.helpers.torch_utils.losses import FinalStepMSE\n",
    "from nnc.helpers.torch_utils.trainers import NODECTrainer\n",
    "\n",
    "\n",
    "from nnc.controllers.neural_network.nnc_controllers import NNCDynamics\n",
    "from nnc.helpers.torch_utils.nn_architectures.fully_connected import StackedDenseTimeControl\n",
    "\n",
    "from plotly import graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph and dynamics parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data_folder = '../../../../data/parameters/ct_lti/'\n",
    "graph='tree' # please use one of the following: lattice, ba, tree\n",
    "device = 'cuda:0'\n",
    "\n",
    "results_data_folder = '../../../../results/ct_lti/multi_sample/'+graph + '/'\n",
    "os.makedirs(results_data_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph data\n",
    "\n",
    "graph_folder = experiment_data_folder+graph+'/'\n",
    "adj_matrix = torch.load(graph_folder+'adjacency.pt').to(dtype=torch.float, device=device)\n",
    "n_nodes = adj_matrix.shape[0]\n",
    "drivers = torch.load(graph_folder + 'drivers.pt')\n",
    "n_drivers = len(drivers)\n",
    "pos = pd.read_csv(graph_folder + 'pos.csv').set_index('index').values\n",
    "driver_matrix = drivers_to_tensor(n_nodes, drivers).to(device)\n",
    "\n",
    "# select dynamics type and initial-target states\n",
    "\n",
    "dyn = ContinuousTimeInvariantDynamics(adj_matrix, driver_matrix)\n",
    "\n",
    "target_states = torch.load(graph_folder+'target_states.pt').to(device)\n",
    "initial_states = torch.load(experiment_data_folder+'init_states.pt').to(device)\n",
    "\n",
    "# total time for control\n",
    "\n",
    "total_time=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate all baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all sample indices\n",
    "for i in tqdm(range(initial_states.shape[0])):\n",
    "\n",
    "    current_sample_id = i\n",
    "    # load current sample\n",
    "    x0 = initial_states[current_sample_id].unsqueeze(0)\n",
    "    xstar = target_states[current_sample_id].unsqueeze(0)\n",
    "    \n",
    "    # calculate optimal control\n",
    "    oc = ControllabiltyGrammianController(\n",
    "        adj_matrix,\n",
    "        driver_matrix,\n",
    "        total_time,\n",
    "        x0,\n",
    "        xstar,\n",
    "        simpson_evals=100,\n",
    "        progress_bar=tqdm,\n",
    "        use_inverse=False,\n",
    "    )\n",
    "    \n",
    "    # OC evaluations for different interaciton intervals.\n",
    "    loss_fn = FinalStepMSE(xstar, total_time=total_time)\n",
    "    all_n_interactions = [50, 500, 5000]\n",
    "    for n_interactions in all_n_interactions:\n",
    "        oc_evaluator = FixedInteractionEvaluator(\n",
    "            'oc_sample'+str(current_sample_id)+'_ninter_' + str(n_interactions),\n",
    "            log_dir=results_data_folder,\n",
    "            n_interactions=n_interactions,\n",
    "            loss_fn=loss_fn,\n",
    "            ode_solver=None,\n",
    "            ode_solver_kwargs={'method' : 'dopri5'},\n",
    "            preserve_intermediate_states=False,\n",
    "            preserve_intermediate_controls=True,\n",
    "            preserve_intermediate_times=False,\n",
    "            preserve_intermediate_energies=False,\n",
    "            preserve_intermediate_losses=False,\n",
    "            preserve_params=False,\n",
    "        )\n",
    "        oc_res = oc_evaluator.evaluate(dyn, oc, x0, total_time, epoch=0)\n",
    "        oc_evaluator.write_to_file(oc_res)\n",
    "        # neural network controller\n",
    "        \n",
    "        \n",
    "    # prepare neural network.\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    nn = StackedDenseTimeControl(n_nodes, \n",
    "                                 n_drivers, \n",
    "                                 n_hidden=0,#1, \n",
    "                                 hidden_size=15,#*n_nodes,\n",
    "                                 activation=torch.nn.functional.elu,\n",
    "                                 use_bias=True\n",
    "                                ).to(x0.device)\n",
    "    nndyn = NNCDynamics(dyn, nn).to(x0.device)\n",
    "    \n",
    "    nn_trainer = NODECTrainer(\n",
    "        nndyn,\n",
    "        x0,\n",
    "        xstar,\n",
    "        total_time,\n",
    "        obj_function=None,\n",
    "        optimizer_class = torch.optim.LBFGS,\n",
    "        optimizer_params=dict(lr=1.2,\n",
    "                              #momentum =0.5\n",
    "                              max_iter=1,\n",
    "                              max_eval=1,\n",
    "                              history_size=100\n",
    "                             ),\n",
    "        ode_solver_kwargs=dict(method='dopri5'),\n",
    "        logger=None,\n",
    "        closure=None,\n",
    "        use_adjoint=False,\n",
    "    )\n",
    "    \n",
    "    # here we initialize with Xavier which seemed to help NODEC converge faster for tree/ba graphs\n",
    "    for name, param in nn.named_parameters():\n",
    "        if len(param.shape) > 1:\n",
    "            torch.nn.init.xavier_normal_(param)\n",
    "            \n",
    "    # here we use higher decelaration rate, which seemed to help NODEC converge faster for tree/ba graphs\n",
    "    # train for 100 epochs\n",
    "    nndyn = nn_trainer.train_best(epochs=100, \n",
    "                                  lr_acceleration_rate=0,\n",
    "                                  lr_deceleration_rate=0.99,\n",
    "                                  loss_variance_tolerance=10,\n",
    "                                  verbose=True\n",
    "                                 )\n",
    "    \n",
    "    # Evaluate after 100 epochs of training for 50 interactions.\n",
    "    nn_logger_50 = FixedInteractionEvaluator('nn_sample_'+str(current_sample_id)+'_train_50',\n",
    "                                             log_dir=results_data_folder,\n",
    "                                             n_interactions=50,\n",
    "                                             loss_fn=loss_fn,\n",
    "                                             ode_solver=None,\n",
    "                                             ode_solver_kwargs={'method' : 'dopri5'},\n",
    "                                             preserve_intermediate_states=False,\n",
    "                                             preserve_intermediate_controls=False,\n",
    "                                             preserve_intermediate_times=False,\n",
    "                                             preserve_intermediate_energies=False,\n",
    "                                             preserve_intermediate_losses=False,\n",
    "                                             preserve_params=True,\n",
    "                                            )\n",
    "    nn_res = nn_logger_50.evaluate(dyn, nndyn.nnc, x0, total_time, epoch=100)\n",
    "    nn_logger_50.write_to_file(nn_res)\n",
    "    \n",
    "    # keep training for 2400 epochs\n",
    "    nndyn = nn_trainer.train_best(epochs=2400, \n",
    "                      lr_acceleration_rate=0,\n",
    "                      lr_deceleration_rate=0.99,\n",
    "                      loss_variance_tolerance=10,\n",
    "                      verbose=True)\n",
    "    \n",
    "    # evaluate for 500 interactions\n",
    "    nn_logger_500 = FixedInteractionEvaluator(\n",
    "            'nn_sample_'+str(current_sample_id)+'_train_500',\n",
    "            log_dir=results_data_folder,\n",
    "            n_interactions=500,\n",
    "            loss_fn=loss_fn,\n",
    "            ode_solver=None,\n",
    "            ode_solver_kwargs={'method' : 'dopri5'},\n",
    "            preserve_intermediate_states=False,\n",
    "            preserve_intermediate_controls=False,\n",
    "            preserve_intermediate_times=False,\n",
    "            preserve_intermediate_energies=False,\n",
    "            preserve_intermediate_losses=False,\n",
    "            preserve_params=False,\n",
    "        )\n",
    "    nn_res = nn_logger_500.evaluate(dyn, nndyn.nnc, x0, total_time, epoch=2500)\n",
    "    nn_logger_500.write_to_file(nn_res)\n",
    "    \n",
    "    # evaluate for 5000 interactions\n",
    "    nn_logger_5000= FixedInteractionEvaluator(\n",
    "            'nn_sample_'+str(current_sample_id)+'_train_5000',\n",
    "            log_dir=results_data_folder,\n",
    "            n_interactions=5000,\n",
    "            loss_fn=loss_fn,\n",
    "            ode_solver=None,\n",
    "            ode_solver_kwargs={'method' : 'dopri5'},\n",
    "            preserve_intermediate_states=False,\n",
    "            preserve_intermediate_controls=False,\n",
    "            preserve_intermediate_times=False,\n",
    "            preserve_intermediate_energies=False,\n",
    "            preserve_intermediate_losses=False,\n",
    "            preserve_params=True,\n",
    "        )\n",
    "    nn_res = nn_logger_5000.evaluate(dyn, nndyn.nnc, x0, total_time, epoch=2500)\n",
    "    nn_logger_5000.write_to_file(nn_res)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
